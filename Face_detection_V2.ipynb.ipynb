{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_to_json(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    json_data = {\n",
    "        \"folder\": root.find(\"folder\").text,\n",
    "        \"filename\": root.find(\"filename\").text,\n",
    "        \"path\": root.find(\"path\").text,\n",
    "        \"source\": {\n",
    "            \"database\": root.find(\"source/database\").text\n",
    "        },\n",
    "        \"size\": {\n",
    "            \"width\": int(root.find(\"size/width\").text),\n",
    "            \"height\": int(root.find(\"size/height\").text),\n",
    "            \"depth\": int(root.find(\"size/depth\").text)\n",
    "        },\n",
    "        \"segmented\": int(root.find(\"segmented\").text),\n",
    "        \"objects\": []\n",
    "    }\n",
    "    \n",
    "    for obj in root.findall(\"object\"):\n",
    "        json_data[\"objects\"].append({\n",
    "            \"name\": obj.find(\"name\").text,\n",
    "            \"pose\": obj.find(\"pose\").text,\n",
    "            \"truncated\": int(obj.find(\"truncated\").text),\n",
    "            \"difficult\": int(obj.find(\"difficult\").text),\n",
    "            \"bndbox\": {\n",
    "                \"xmin\": int(obj.find(\"bndbox/xmin\").text),\n",
    "                \"ymin\": int(obj.find(\"bndbox/ymin\").text),\n",
    "                \"xmax\": int(obj.find(\"bndbox/xmax\").text),\n",
    "                \"ymax\": int(obj.find(\"bndbox/ymax\").text)\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    return json_data\n",
    "\n",
    "def convert_annotations(annotations_folder, output_folder, output_images_folder):\n",
    "    # Create the output folders if they don't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    os.makedirs(output_images_folder, exist_ok=True)\n",
    "    \n",
    "    for xml_file in os.listdir(annotations_folder):\n",
    "        if xml_file.endswith(\".xml\"):\n",
    "            xml_path = os.path.join(annotations_folder, xml_file)\n",
    "            tree = ET.parse(xml_path)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            # Check if the XML file has only one bounding box\n",
    "            if len(root.findall(\"object\")) == 1:\n",
    "                json_data = xml_to_json(xml_path)\n",
    "                \n",
    "                json_file = xml_file.replace(\".xml\", \".json\")\n",
    "                json_path = os.path.join(output_folder, json_file)\n",
    "                \n",
    "                with open(json_path, \"w\") as f:\n",
    "                    json.dump(json_data, f, indent=4)\n",
    "                \n",
    "                print(f\"Converted {xml_file} to {json_file}\")\n",
    "                \n",
    "                # Get the image path from the JSON data and add the prefix\n",
    "                image_path = os.path.join(\"wider_face/data\", json_data[\"path\"])\n",
    "                image_file = os.path.basename(image_path)\n",
    "                output_image_path = os.path.join(output_images_folder, image_file)\n",
    "                \n",
    "                # Copy the image file to the output folder\n",
    "                shutil.copy2(image_path, output_image_path)\n",
    "                print(f\"Copied {image_file} to {output_images_folder}\")\n",
    "\n",
    "# Usage example\n",
    "annotations_folder = \"wider_face/data/WIDER_train_annotations/\"\n",
    "output_folder = \"data_image/annotation/\"\n",
    "output_images_folder = \"data_image/image/\"\n",
    "convert_annotations(annotations_folder, output_folder, output_images_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Set the paths for the source folder and destination folders\n",
    "source_folder = \"data_image/image\"\n",
    "train_folder = \"data_image/train/images\"\n",
    "test_folder = \"data_image/test/images\"\n",
    "val_folder = \"data_image/val/images\"\n",
    "\n",
    "# Get a list of all image files in the source folder\n",
    "image_files = [f for f in os.listdir(source_folder) if f.endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "\n",
    "# Shuffle the image files randomly\n",
    "random.shuffle(image_files)\n",
    "\n",
    "# Calculate the number of images for each split\n",
    "total_images = len(image_files)\n",
    "train_size = int(0.7 * total_images)\n",
    "test_size = int(0.2 * total_images)\n",
    "val_size = total_images - train_size - test_size\n",
    "\n",
    "# Split the image files into train, test, and validation sets\n",
    "train_files = image_files[:train_size]\n",
    "test_files = image_files[train_size:train_size+test_size]\n",
    "val_files = image_files[train_size+test_size:]\n",
    "\n",
    "# Copy the images to the respective folders\n",
    "for file in train_files:\n",
    "    shutil.copy(os.path.join(source_folder, file), train_folder)\n",
    "for file in test_files:\n",
    "    shutil.copy(os.path.join(source_folder, file), test_folder)  \n",
    "for file in val_files:\n",
    "    shutil.copy(os.path.join(source_folder, file), val_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in ['train','test','val']:\n",
    "    for file in os.listdir(os.path.join('data_image', folder, 'images')):\n",
    "        \n",
    "        filename = file.split('.')[0]+'.json'\n",
    "        existing_filepath = os.path.join('data_image','annotation', filename)\n",
    "        if os.path.exists(existing_filepath): \n",
    "            new_filepath = os.path.join('data_image',folder,'labels',filename)\n",
    "            os.replace(existing_filepath, new_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(os.path.join('data_image/train/images/0_Parade_marchingband_1_431.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('data_image/train/labels/0_Parade_marchingband_1_431.json'), 'r') as f:\n",
    "    label = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the bounding box coordinates\n",
    "x_min = label['objects'][0]['bndbox']['xmin']\n",
    "y_min = label['objects'][0]['bndbox']['ymin']\n",
    "x_max = label['objects'][0]['bndbox']['xmax']\n",
    "y_max = label['objects'][0]['bndbox']['ymax']\n",
    "\n",
    "# Get the image dimensions\n",
    "height, width = img.shape[:2]\n",
    "\n",
    "# Normalize the bounding box coordinates to the range [0.0, 1.0]\n",
    "bbox = [x_min / width, y_min / height, x_max / width, y_max / height]\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.RandomCrop(width=500, height=500),\n",
    "    A.HorizontalFlip(p=0.5), \n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.RandomGamma(p=0.2), \n",
    "    A.RGBShift(p=0.2), \n",
    "    A.VerticalFlip(p=0.5)\n",
    "], bbox_params=A.BboxParams(format='albumentations', label_fields=['class_labels']))\n",
    "\n",
    "transformed = transform(image=img, bboxes=[bbox], class_labels=['face'])\n",
    "\n",
    "cropped_img = transformed['image']\n",
    "cropped_labels = transformed['bboxes']\n",
    "\n",
    "# Convert the cropped image from BGR to RGB color space\n",
    "cropped_img_rgb = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Extract the cropped bounding box coordinates\n",
    "if cropped_labels:\n",
    "    x_min, y_min, x_max, y_max = cropped_labels[0]\n",
    "\n",
    "    # Convert the normalized coordinates back to pixel values\n",
    "    x_min = int(x_min * 500)\n",
    "    y_min = int(y_min * 500)\n",
    "    x_max = int(x_max * 500)\n",
    "    y_max = int(y_max * 500)\n",
    "\n",
    "    # Draw the cropped bounding box on the cropped image\n",
    "    cv2.rectangle(cropped_img_rgb, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "\n",
    "# Display the cropped image with the bounding box using Matplotlib\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(cropped_img_rgb)\n",
    "plt.axis('off')\n",
    "plt.title('Cropped Image with Bounding Box')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for partition in ['train','test','val']: \n",
    "    for image in os.listdir(os.path.join('data_image', partition, 'images')):\n",
    "        img = cv2.imread(os.path.join('data_image', partition, 'images', image))\n",
    "\n",
    "        coords = [0,0,0.00001,0.00001]\n",
    "        label_path = os.path.join('data_image', partition, 'labels', f'{image.split(\".\")[0]}.json')\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                label = json.load(f)\n",
    "\n",
    "            # Extract the bounding box coordinates\n",
    "            x_min = label['objects'][0]['bndbox']['xmin']\n",
    "            y_min = label['objects'][0]['bndbox']['ymin']\n",
    "            x_max = label['objects'][0]['bndbox']['xmax']\n",
    "            y_max = label['objects'][0]['bndbox']['ymax']\n",
    "\n",
    "            # Get the image dimensions\n",
    "            height, width = img.shape[:2]\n",
    "\n",
    "            # Normalize the bounding box coordinates to the range [0.0, 1.0]\n",
    "            bbox = [x_min / width, y_min / height, x_max / width, y_max / height]\n",
    "\n",
    "        try: \n",
    "            for x in range(5):\n",
    "                augmented = transform(image=img, bboxes=[bbox], class_labels=['face'])\n",
    "                cv2.imwrite(os.path.join('aug_data_wider', partition, 'images', f'{image.split(\".\")[0]}.{x}.jpg'), augmented['image'])\n",
    "\n",
    "                annotation = {}\n",
    "                annotation['image'] = image\n",
    "\n",
    "                if os.path.exists(label_path):\n",
    "                    if len(augmented['bboxes']) == 0: \n",
    "                        annotation['bbox'] = [0,0,0,0]\n",
    "                        annotation['class'] = 0 \n",
    "                    else: \n",
    "                        annotation['bbox'] = augmented['bboxes'][0]\n",
    "                        annotation['class'] = 1\n",
    "                else: \n",
    "                    annotation['bbox'] = [0,0,0,0]\n",
    "                    annotation['class'] = 0 \n",
    "\n",
    "\n",
    "                with open(os.path.join('aug_data_wider', partition, 'labels', f'{image.split(\".\")[0]}.{x}.json'), 'w') as f:\n",
    "                    json.dump(annotation, f)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(file_path):\n",
    "    image = tf.io.read_file(file_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = tf.data.Dataset.list_files('aug_data_wider/train/images/*.jpg', shuffle=False)\n",
    "train_images = train_images.map(load_image)\n",
    "train_images = train_images.map(lambda x: tf.image.resize(x, (120, 120)))\n",
    "train_images = train_images.map(lambda x: x / 255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = tf.data.Dataset.list_files('aug_data_wider/test/images/*.jpg', shuffle=False)\n",
    "test_images = test_images.map(load_image)\n",
    "test_images = test_images.map(lambda x: tf.image.resize(x,(120,120)))\n",
    "test_images = test_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images = tf.data.Dataset.list_files('aug_data_wider/val/images/*.jpg', shuffle=False)\n",
    "val_images = val_images.map(load_image)\n",
    "val_images = val_images.map(lambda x: tf.image.resize(x,(120,120)))\n",
    "val_images = val_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "def load_labels(label_path):\n",
    "    with open(label_path.numpy(), 'r', encoding=\"utf-8\") as f:\n",
    "        label = json.load(f)\n",
    "    return [label['class']], label['bbox']\n",
    "\n",
    "def load_labels_wrapper(label_path):\n",
    "    output = tf.py_function(load_labels, [label_path], [tf.uint8, tf.float16])\n",
    "    # Directly set the shapes of the tensors without unpacking\n",
    "    output[0].set_shape([1])  # Setting the shape for class label tensor\n",
    "    output[1].set_shape([4])  # Assuming bbox contains 4 elements [x_min, y_min, x_max, y_max]\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = tf.data.Dataset.list_files('aug_data_wider/train/labels/*.json', shuffle=False)\n",
    "train_labels = train_labels.map(load_labels_wrapper)\n",
    "\n",
    "train_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = tf.data.Dataset.list_files('aug_data_wider/test/labels/*.json', shuffle=False)\n",
    "test_labels = test_labels.map(load_labels_wrapper)\n",
    "\n",
    "test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = tf.data.Dataset.list_files('aug_data_wider/val/labels/*.json', shuffle=False)\n",
    "val_labels = val_labels.map(load_labels_wrapper)\n",
    "\n",
    "val_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_images), len(train_labels), len(test_images), len(test_labels), len(val_images), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.data.Dataset.zip((train_images, train_labels))\n",
    "train = train.shuffle(5000)\n",
    "train = train.batch(8)\n",
    "train = train.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tf.data.Dataset.zip((test_images, test_labels))\n",
    "test = test.shuffle(1300)\n",
    "test = test.batch(8)\n",
    "test = test.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = tf.data.Dataset.zip((val_images, val_labels))\n",
    "val = val.shuffle(1000)\n",
    "val = val.batch(8)\n",
    "val = val.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.as_numpy_iterator().next()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = train.as_numpy_iterator()\n",
    "res = data_samples.next()\n",
    "\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx in range(4): \n",
    "    sample_image = res[0][idx]\n",
    "    sample_coords = res[1][1][idx]\n",
    "    \n",
    "    # Create a copy of the sample_image\n",
    "    sample_image_copy = np.copy(sample_image)\n",
    "    \n",
    "    cv2.rectangle(sample_image_copy, \n",
    "                  tuple(np.multiply(sample_coords[:2], [120,120]).astype(int)),\n",
    "                  tuple(np.multiply(sample_coords[2:], [120,120]).astype(int)), \n",
    "                        (255,0,0), 2)\n",
    "\n",
    "    ax[idx].imshow(sample_image_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Build Deep Learning using the Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Import Layers and Base Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, GlobalMaxPooling2D\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG16(include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Build instance of Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape=(120, 120, 3)):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    base_model = VGG16(include_top=False, input_tensor=input_layer)\n",
    "    \n",
    "    # Feature extraction\n",
    "    pooled_features = GlobalMaxPooling2D()(base_model.output)\n",
    "    \n",
    "    # Classification head\n",
    "    classifier_output = Dense(2048, activation='relu')(pooled_features)\n",
    "    classifier_output = Dense(1, activation='sigmoid', name='class_output')(classifier_output)\n",
    "    \n",
    "    # Regression head\n",
    "    regressor_output = Dense(2048, activation='relu')(pooled_features)\n",
    "    regressor_output = Dense(4, activation='sigmoid', name='bbox_output')(regressor_output)\n",
    "    \n",
    "    # Full model\n",
    "    model = Model(inputs=input_layer, outputs=[classifier_output, regressor_output])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localization_loss(y_true, yhat):            \n",
    "    delta_coord = tf.reduce_sum(tf.square(y_true[:,:2] - yhat[:,:2]))\n",
    "                  \n",
    "    h_true = y_true[:,3] - y_true[:,1] \n",
    "    w_true = y_true[:,2] - y_true[:,0] \n",
    "\n",
    "    h_pred = yhat[:,3] - yhat[:,1] \n",
    "    w_pred = yhat[:,2] - yhat[:,0] \n",
    "    \n",
    "    delta_size = tf.reduce_sum(tf.square(w_true - w_pred) + tf.square(h_true-h_pred))\n",
    "    \n",
    "    return delta_coord + delta_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the model\n",
    "facetracker = build_model()\n",
    "facetracker.summary()\n",
    "batches_per_epoch = len(train)\n",
    "lr_decay = (1. / 0.75 - 1) / batches_per_epoch\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, decay=lr_decay)\n",
    "\n",
    "facetracker.compile(optimizer=optimizer,\n",
    "                    loss={'class_output': tf.keras.losses.BinaryCrossentropy(),\n",
    "                          'bbox_output': localization_loss},\n",
    "                    metrics={'class_output': ['accuracy'], 'bbox_output': []})\n",
    "\n",
    "# Callbacks and training\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='logs')\n",
    "hist = facetracker.fit(train, epochs=10, validation_data=val, callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 Plot Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extracting data from the training history\n",
    "history_dict = hist.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "acc_values = history_dict['class_output_accuracy']  # This might differ based on your specific outputs\n",
    "val_acc_values = history_dict['val_class_output_accuracy']  # This might differ too\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "# Plotting the training and validation loss\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, loss_values, label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting the training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, acc_values, label='Training accuracy')\n",
    "plt.plot(epochs, val_acc_values, label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1 Make Predictions on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = test_data.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = facetracker.predict(test_sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx in range(4): \n",
    "    sample_image = test_sample[0][idx].copy()  # Create a copy of the image array\n",
    "    sample_coords = yhat[1][idx]\n",
    "    \n",
    "    if yhat[0][idx] > 0.9:\n",
    "        cv2.rectangle(sample_image, \n",
    "                      tuple(np.multiply(sample_coords[:2], [120,120]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [120,120]).astype(int)), \n",
    "                            (255,0,0), 2)\n",
    "    \n",
    "    ax[idx].imshow(sample_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker.save('facetracker_wider.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker = load_model('facetracker_wider.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3 Real Time Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    _ , frame = cap.read()\n",
    "    \n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    resized = tf.image.resize(rgb, (120,120))\n",
    "    \n",
    "    yhat = facetracker.predict(np.expand_dims(resized/255,0))\n",
    "    sample_coords = yhat[1][0]\n",
    "    \n",
    "    if yhat[0] > 0.5: \n",
    "        # Controls the main rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.multiply(sample_coords[:2], [frame.shape[1],frame.shape[0]]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [frame.shape[1],frame.shape[0]]).astype(int)), \n",
    "                            (255,0,0), 2)\n",
    "        # Controls the label rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [frame.shape[1],frame.shape[0]]).astype(int), \n",
    "                                    [0,-30])),\n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [frame.shape[1],frame.shape[0]]).astype(int),\n",
    "                                    [80,0])), \n",
    "                            (255,0,0), -1)\n",
    "        \n",
    "        # Controls the text rendered\n",
    "        cv2.putText(frame, 'face', tuple(np.add(np.multiply(sample_coords[:2], [frame.shape[1],frame.shape[0]]).astype(int),\n",
    "                                               [0,-5])),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow('EyeTrack', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
